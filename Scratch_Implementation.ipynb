{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f77f69a-ff30-4e66-a371-6d385dc14057",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# create a toy dataset\n",
    "X = np.random.rand(4,2)\n",
    "y = np.array([[0], [1], [1], [0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a85e57-12df-4560-91c4-84a6fe8ef8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalise parameters\n",
    "def init_params(input_neurons,hidden_neurons,output_neurons):\n",
    "    np.random.seed(0)\n",
    "    W1 = np.random.rand(hidden_neurons,input_neurons)\n",
    "    B1 = np.random.rand(hidden_neurons,1)\n",
    "    W2 = np.random.rand(output_neurons,hidden_neurons)\n",
    "    B2 = np.random.rand(output_neurons,1)\n",
    "    parameters = {\"W1\":W1, \"B1\":B1, \"W2\":W2, \"B2\":B2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "# Define activation function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Define forward prop\n",
    "def forward_prop(X,parameters):\n",
    "    W1,B1,W2,B2 = parameters.values()\n",
    "\n",
    "    # Compute activations of hidden layer\n",
    "    Z1 = np.dot(W1,X.T) + B1\n",
    "    A1 = sigmoid(Z1)\n",
    "\n",
    "    # Compute activations of output layer\n",
    "    Z2 = np.dot(W2,A1) + B2\n",
    "    A2 = sigmoid(Z2)\n",
    "\n",
    "    # Store intermediate values later used for back prop\n",
    "    intermed_vals = {\"Z1\":Z1, \"A1\":A1, \"Z2\":Z2, \"A2\":A2}\n",
    "\n",
    "    return A2, intermed_vals\n",
    "\n",
    "# Compute loss function\n",
    "def binary_cross_entropy_loss(A2,y):\n",
    "    m = y.shape[0]\n",
    "    loss = -1/m * np.sum(-y * np.log(A2) - (1-y) * np.log(1-A2))\n",
    "    return loss\n",
    "\n",
    "# Back Propogation\n",
    "def back_prop(parameters,intermed_vals,X,y):\n",
    "    Z1 = intermed_vals[\"Z1\"]\n",
    "    A1 = intermed_vals[\"A1\"]\n",
    "    Z2 = intermed_vals[\"Z2\"]\n",
    "    A2 = intermed_vals[\"A2\"]\n",
    "\n",
    "    # Compute derivative of loss with respect to A2\n",
    "    dA2 = -(y.T/A2) + (1-y.T)/(1-A2)\n",
    "\n",
    "    # Compute derivative of Z2\n",
    "    dZ2 = dA2 * (A2 * (1-A2))\n",
    "\n",
    "    # Compute derivatives of weights and bias in outer layer\n",
    "    dW2 = (1/m) * np.dot(dZ2,A1.T)\n",
    "    dB2 = (1/m) * np.sum(dZ2)\n",
    "\n",
    "    # Compute the derivatives of A1 and Z1 with respect to the loss\n",
    "    dA1 = np.dot(parameters[\"W2\"].T,dZ2)\n",
    "    dZ1 = dA1 * A1 * (1-A1)\n",
    "\n",
    "    # Compute derivatives of weights and bias in hidden layer\n",
    "    dW1 = (1/m) * np.dot(dZ1,X) \n",
    "    dB1 = (1/m) * np.sum(dZ1)\n",
    "\n",
    "    gradients = {\"DW1\":dW1, \"DB1\":dB1, \"DW2\":dW2, \"DB2\":dB2}\n",
    "    return gradients\n",
    "\n",
    "# Update parameters\n",
    "def update_params(parameters,gradients,learning_rate):\n",
    "    # Retrieve Gradients\n",
    "    dW1 = gradients[\"DW1\"]\n",
    "    dB1 = gradients[\"DB1\"]\n",
    "    dW2 = gradients[\"DW2\"]\n",
    "    dB2 = gradients[\"DB2\"]\n",
    "    \n",
    "    # Retrieve parameters\n",
    "    W1 = parameters[\"W1\"]\n",
    "    B1 = parameters[\"B1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    B2 = parameters[\"B2\"]\n",
    "\n",
    "    # Update\n",
    "    W1 = W1 - learning_rate * dW1\n",
    "    B1 = B1 - learning_rate * dB1\n",
    "    W2 = W2 - learning_rate * dW2\n",
    "    B2 = B2 - learning_rate * dB2\n",
    "\n",
    "    parameters = {\"W1\":W1, \"B1\":B1, \"W2\":W2, \"B2\":B2}\n",
    "    return parameters\n",
    "\n",
    "# Train neural network\n",
    "def train(X,y,hidden_layer_size, num_iterations, learning_rate):\n",
    "    \n",
    "    parameters = init_params(X.shape[1],hidden_layer_size,1)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Forward prop\n",
    "        A2, intermed_vals = forward_prop(X,parameters)\n",
    "        \n",
    "        # Loss\n",
    "        loss = binary_cross_entropy_loss(A2,y)\n",
    "\n",
    "        # Back Prop\n",
    "        gradients = back_prop(parameters,intermed_vals,X,y)\n",
    "\n",
    "        # Update Params\n",
    "        parameters = update_params(parameters,gradients,learning_rate)\n",
    "        \n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration {i}, Loss = {loss}\")\n",
    "\n",
    "    # Return optimal parameters\n",
    "    return parameters\n",
    "\n",
    "# Predictions\n",
    "def predict(X,parameters):\n",
    "    predictions = forward_prop(X,parameters)\n",
    "    return predictions      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3f6d4-aa4c-4d4c-9c99-d5d99f5d039c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute predictions\n",
    "parameters = train(X,y,hidden_layer_size = 3,num_iterations = 1000,learning_rate = 1)\n",
    "predictions,_ = predict(X,parameters)\n",
    "predictions > 0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
